\subsection{Clustering Minimal Description Length}
\label{ssc:mdl-clust}

In this section we analyze changes in the LM minimal description length values
when it is computed for a collection of clusters of arbitrary dimension and size
-- clustering.

For our experiments we performed clustering of the climate dataset,
see Section \ref{ssc:climate}. We performed multiple clusterings of the dataset
varying only one parameter -- \emph{best\_bound}. This parameter affect
cluster partitioning mechanism in LMCLUS algorithm which influence a final
number of generated clusters \cite{Haralick:2007rt}. We generate a sample
composed of clusterings produced by changing \emph{best\_bound} parameter in
range [0.1, 1.2] with value increment of 0.05. This results in 23 clusterings
per each clustering sample. For a statistical evaluation, we collected 1000
clustering sample, total 23000 clusterings.

For the reference to the K{\"o}ppen-Geiger (KG) climate classification system that
identifies 34 climate classes, we set \emph{number\_of\_clusters} parameter,
an expected number of clusters for the LMCLUS algorithm, to 34.
This parameter affects sampling procedure for a manifold basis of a prospective
cluster \cite{Haralick:2007rt}.

In our experiments, we also set minimum cluster size parameter,
\emph{min\_cluster\_size}, to 150, and sampling factor parameter,
\emph{sampling\_factor}, which is used in the sampling heuristics to constrain
a number of samples for potential cluster linear manifold, to 1.0.
Rest of the LMCLUS algorithm parameters where set to default values.
For more detailed description of LMCLUS algorithm parameters consult
Table~\ref{tab:lmclus-params} in the Appendix~\ref{ssec:lmclus-params}.

% Setup
<<echo=false;cache=false>>=
# load modules
using DataFrames
using Query
using PLplot
using JLD
PLplot.set_default_device(:pdfcairo)
joinpath(pwd(), "src/plot-utils.jl") |> include # additional plots

# plot bootstrapped average dimension of clusters in clustering of size 8
import Bootstrap # for Bootstrap CI for estimation
function bootci(data; α=0.05, boot=1500)
    bs = Bootstrap.bootstrap(data, mean, Bootstrap.BasicSampling(boot))
    bci = Bootstrap.ci(bs, Bootstrap.BasicConfInt(1-α)) |> first
    return [bci...]
end

# load data for analysis
X = load("data/cl-data.jld", "data")
N, d = size(X) #17781.0 # size of the dataset
data = readtable("data/cl-mdl.csv");
@

\subsubsection{Parameters of Clusterings}
\label{sssc:clust-params}

We analyzed a clustering size, which is a number of clusters in the clustering,
gathered from generated clustering samples. First, we group all clusterings by
their sizes with respect to \emph{best\_bound} parameter.

<<echo=false; cache=false; fig_pos="H"; out_width="5.5in"; label="best-bound-cls"; fig_cap="Size of clusterings (number of clusters in the clustering) produced by LMCLUS algorithm by varying \\emph{best\\_bound} parameter value.">>=
# Plot number of clusters vs 'best_bound' parameter values
#
# Pull together clusterings of the same size
clsize_bb = by(data, :ClusterID,
               df->DataFrame(ClusteringSize=length(df[:ClusterID]), BBound=first(df[:BBound])))
medclsize_bb = by(clsize_bb, :BBound, df -> DataFrame(MedClusteringSize = median(df[:ClusteringSize])))

# and plot
clssize_by_bb = @from r in clsize_bb begin
    @group r.ClusteringSize by r.BBound into g
    @select get(g.key) => convert(Vector{Float64}, map(get,  g))
    @collect Dict
end

# same as above but a violin plot
# draw(width=720, height=480) do opt
#     violin(clssize_by_bb, trim=false, bw=0.4, width=2.)
#     PLplot.labels("Best Bound", "Number of clusters in a clustering", "")
# end

draw(width=720, height=480) do opt
    boxplot(clssize_by_bb, quartile=:hinges)
    PLplot.labels("Best Bound", "Number of clusters in a clustering", "")
end
@

Figure~\ref{fig:best-bound-cls} shows a large variation for clustering sizes
for \emph{best\_bound} in range [0.4,0.7]. This range behaves as a transition
zone between clusterings with a small number of clusters, corresponding to
the large \emph{best\_bound} value, and a large number of clusters,
corresponding to the small \emph{best\_bound} value. We can speculate that for
current dataset algorithm is not able to pick mid range clusters, resulting
in arbitrary partitioning of some large clusters.

<<echo=false; cache=false; fig_pos="H"; out_width="4.5in"; label="best-bound-cl-34"; fig_cap="The \\emph{best\\_bound} parameter value distribution of clustering of size 34.">>=
# best bound by  clustering sizes
bb_by_clssize = @from r in clsize_bb begin
    @group r.BBound by r.ClusteringSize into g
    @select Float64(get(g.key)) => convert(Vector{Float64}, map(get,  g))
    @collect Dict
end

# plot `best_bound` parameter values for clusterings with 34 clusters
clustsize = 34

draw() do opts
    PLplot.histogram(bb_by_clssize[clustsize], bins=4)
    PLplot.labels("Best Bound", "Count", "Best Bound Parameter Distribution for Clustering of Size $clustsize")
end
@

Figure~\ref{fig:best-bound-cl-34} shows a distribution of the \emph{best\_bound}
parameter for the clustering size equal to 34, which correspond to the number of
climate zones in K{\"o}ppen-Geiger classification. We can observe that
\emph{best\_bound} parameter is distributed within a "transitional" range where
the clustering algorithm produces clusterings with wide variation of sizes for
given parameters, see Figure~\ref{fig:best-bound-cls}.


Next, we analyze dimensionality of clusters from the sampled clusterings.
Results showed that majority of the clusters in various sized clusterings are
1-dimensional. Usually, the last cluster has dimension zero which is by design -
this cluster is a collection of all outliers that algorithm wasn't able to
associate with any known cluster.
Moreover, a cluster, before last one in a clustering of any size,
has higher dimension then one. This behavior can be attributed to constrained
minimal cluster size, forcing algorithm to generate high-dimensional clusters
with that would pass minimal cluster size threshold, see discussion in
the beginning of Section~\ref{ssc:mdl-clust}.

<<echo=false; cache=false; fig_pos="H"; out_width="6.0in"; label="mdl-clust-cdim"; fig_cap="The \\emph{best\\_bound} parameter value distribution of clustering of size 34.">>=
# group clusters in clusterings, and into cluster dimension distributions
cldims = @from r in data begin
    @group r by r.ClusterID into g
    @let cdim = Vector{Int}([get(s[:ManifoldDim]) for s in g])
    @select {ClusteringSize=length(cdim), ClusterDims = cdim}
    @collect DataFrame
end

# group dimensions of the corresponding clusters in clusterings of the same size
cldim_by_clsize = @from r in cldims begin
    @group r by r.ClusteringSize into g
    @select g.key => hcat(map(e->e[2], g)...)'
    @collect Dict
end;

# plot cluster dimension distribution for clusterings of various size
mean_cldim_by_clsize = Dict([(Float64(k),vec(mean(v,2))) for (k,v) in cldim_by_clsize])
draw(width=800, height=480) do opts
    boxplot(mean_cldim_by_clsize)
    PLplot.abline!(h=1., col=7)
    color!(1)
    PLplot.labels("Clustering Size", "Average Cluster Dimension", "Cluster Dimension Distribution in Various Size Clusterings")
end
@

Figure~\ref{fig:mdl-clust-cdim} shows a cluster dimension distribution for
clusterings of different size. The average dimension of clusters approaches 1
(red line) as number of clusters grows in a clustering. However, there is
a large variability in a cluster dimensionality for small-sized clusterings,
which gradually decreases as the clustering size increases.


In order investigate further occurrence of this pattern, we will look at
the cluster size in generated clusterings. We grouped clusters from
the clusterings of the same size and pooled together corresponded cluster sizes.

<<echo=false; cache=false; fig_pos="H"; out_width="6.0in"; label="clust-size-14"; fig_cap="Cluster size distribution for the clustering of size 14.">>=
# group clusters in clusterings, and into cluster size distributions
cldims = @from r in data begin
    @group r by r.ClusterID into g
    @let cdim = Vector{Int}([get(s[:ClusterSize]) for s in g])
    @select {ClusteringSize=length(cdim), ClusterSize = cdim}
    @collect DataFrame
end
# group dimensions of the corresponding clusters in clusterings of the same size
csize_by_clsize = @from r in cldims begin
    @group r by r.ClusteringSize into g
    @select g.key => hcat(map(e->e[2], g)...)'
    @collect Dict
end;

# plot bootstrapped average cluster size in clustering of with 14 clusters
clustsize = 14
draw(width=800, height=480) do opts
    # clusters size with CI
    boxplot(csize_by_clsize[clustsize], quartile=:tukey)
    # bootstrapped mean clusters size with CI
    bci = mapslices(s->bootci(s,α=0.05) , csize_by_clsize[clustsize], 1)
    plot!(bci[1,:]; typ=:line, pch=Int32(20), col=7)
    ciplot!(bci)
    # minimal cluster size
    PLplot.abline!(h=150, col=5)
    color!(1)
    PLplot.labels("Cluster Number", "Cluster Size", "Total Clusters: $clustsize")
end
@

Figure~\ref{fig:clust-size-14} shows distribution of the cluster sizes for
clusterings with 14 clusters. The red line on a plot is a bootstrapped mean
cluster size with the corresponding 95\% confidence interval, and the blue line
is a minimal cluster size specified by the LMCLUS parameter,
\emph{min\_cluster\_size}, which is set to 150.

As the number of clusters in the clustering grows, the cluster size  approaches
to the minimal cluster size threshold. Moreover, the cluster size gradually
decreases, for from large clusters, generated at the beginning of
the clustering procedure, to the clusters with the size near the threshold limit
for clusters produced near the end of clustering procedure. This behavior is
observed for all sizes of clusterings, especially with few clusters.


\subsubsection{Clustering MDL Calculations}
\label{sssc:clust-mdl}

In our calculations of MDL values for clusterings, we used following parameters:
an encoding model constant, $P_m$, set to 32, an encoding data constant,
$P_d$, set to 16, and the user-defined quantization error upper boundary,
$\varepsilon$, set to 0.001.

<<echo=false;cache=false>>=
# group model MDL-related sample data
clust_mmdl_stats = @from r in data begin
    @group r by r.ClusterID into g
    @let mmdl = map(s->get(s[:ModelDL]), g)
    @select {
        Clustering     = get(g.key),
        ClusteringSize = Int(length(mmdl)),
        ModelDL        = map(s->get(s[:ModelDL]), g)
    }
    @collect DataFrame
end
clust_mmdl_stats[:ClusteringSize] = map(Int,clust_mmdl_stats[:ClusteringSize]);
@

In two-part MDL calculation for linear manifold clusters, first part, a cluster
model description is computed by \eqref{eq:mdl-lmc-model},
$L(\mathcal{M}) = P_m N (N+1)/2$.
However, when cluster is not of a linear manifold form, we calculate its
description as if it's a spherical cluster. Thus, we only need to encode center
of the spherical cluster in a model description, $L(\mathcal{M}) = P_m N$.

<<echo=false; cache=false; fig_pos="H"; out_width="5.0in"; label="model-dl-stats"; fig_cap="Cluster model description length mean amd median values.">>=
# plot cluster model description length from sampled clusterings
modelmdl_by_cls = @from r in clust_mmdl_stats begin
    @orderby r.ClusteringSize
    @group r by r.ClusteringSize into g
    @select {
        ClusteringSize = get(g.key),
        AvgModelDL    = round(mean(g.elements[1][:ModelDL]), 4),
        MedianModelDL = median(g.elements[1][:ModelDL])
    }
    @collect DataFrame
end

draw() do opts
    plot(convert(Vector{Float64}, modelmdl_by_cls[:ClusteringSize]),
         convert(Matrix{Float64}, modelmdl_by_cls[:, [:AvgModelDL, :MedianModelDL]]),
         typ=:line, ymax=9700.
    )
    color!(1)
    PLplot.labels("Number of clusters", "Cluster Model Description Length", "")
    PLplot.legend(["Mean", "Median"],pos = PLplot.POSITION_BOTTOM | PLplot.POSITION_RIGHT)
end
@

Figure~\ref{fig:model-dl-stats} shows mean and median model description length
values for sampled clusterings grouped by their size. The model MDL for
1-dimensional LM cluster is 32 x 24 x 25/2 = 9600. As expected, a median MDL for
a clusters in sampled clusterings is 9600 bits, which corresponds to MDL value
of the dominant cluster in all generated clustering samples. A mean cluster
model MDL value is expected to approach median value. As almost every clustering
contains a 0-dimension cluster of outliers, the more clusters in the clustering,
the closer average model MDL value to the model MDL of 1-dimensional LM cluster.

In context of the MDL calculation for a clustering, the total model MDL of
a clustering $\mathcal{K}$, composed of $k$ clusters, is calculated as

$$L(\mathcal{K}) = \sum_k L(\mathcal{M}_k)$$

Thus, we should expect linear grows of the clustering model MDL value with
a number of clusters, because every cluster has only one 0-dimensional "noise"
cluster and the rest of the clusters are described by constant number of bits,
see Figure~\ref{fig:clustering-model-dl}.

<<echo=false; cache=false; fig_pos="H"; out_width="4.0in"; label="clustering-model-dl"; fig_cap="Clustering Model Description Length.">>=
# plot clustering model description length from samples
totmodelmdl_by_cls = @from r in clust_mmdl_stats begin
    @orderby r.ClusteringSize
    @group r by r.ClusteringSize into g
    @select {
        ClusteringSize = get(g.key),
        ModelDL    = sum(g.elements[1][:ModelDL]),
    }
    @collect DataFrame
end

draw() do opts
    plot(convert(Vector{Float64}, totmodelmdl_by_cls[:ClusteringSize]),
         convert(Vector{Float64}, totmodelmdl_by_cls[:ModelDL]),
         typ=:line,
    )
    PLplot.labels("Number of clusters", "Model Description Length", "")
end
@

In context of the MDL calculation for a clustering, a total data MDL value of
a clustering $\mathcal{K}$, composed of $k$ clusters, is calculated as

\begin{equation}
L(D \;|\;\mathcal{K}) = \sum_k L(\mathcal{C_k} \;|\;\mathcal{M_k}) = \sum_k P_d M_k J_k + \sum_k J_k S_{k, M_k}(\varepsilon)
\end{equation}

where $L(\mathcal{C} \;|\;\mathcal{M})$ is a description length of the cluster
$\mathcal{C}$ encoded given the model $\mathcal{M}$, see \eqref{eq:mdl-lmc-data}.

<<echo=false;cache=false>>=
# group data MDL-related sample data
clust_dmdl_stats = @from r in data begin
    @group r by r.ClusterID into g
    @select {
        Clustering     = get(g.key),
        ClusteringSize = Int(length(g.elements)),
        EntropyAll    = map(s->get(s[:EntropyAll]), g) |> sum,
        EntropyOCS    = map(s->get(s[:EntropyOCS]), g) |> sum,
        ManifoldProjDL = map(s->get(s[:ManifoldProjDL]), g) |> sum,
        DataDL         = map(s->get(s[:ManifoldProjDL])+get(s[:EntropyOCS]), g) |> sum,
    }
    @collect DataFrame
end
clust_dmdl_stats[:ClusteringSize] = map(Int,clust_dmdl_stats[:ClusteringSize]);
@

If size of the dataset $D$ is $d = |D| = \sum_k J_k$, then first term,
$\sum_k P_d M_k J_k$ can be approximated with $(P_d d) E[M]$,
where $E[M]$ is an expected value of the cluster dimensionality in a particular
clustering, and $d = \sum_k J_k$ is a size of the dataset.
Second term, $\sum_k J_k S_{k, M_k}$ can be views as expected values of
the entropy of the OSC points distribution $(d)E[S]$, so

$$L(D \;|\;\mathcal{K}) = (P_d d) E[M] + (d) E[S]$$

We also would like to compare data MDL of the clustering when all components are
just encoded with entropy, as in case of MDL calculation for a zero-dimensional
cluster.

$$L_z(D \;|\;\mathcal{K}) = (d)E[S]$$

Figure~\ref{fig:clustering-data-dl} shows a distribution of the clustering data
MDL values, $L(D \;|\;\mathcal{K})$, as a box plot, and bootstrapped mean value
of $L(D \;|\;\mathcal{K})$, as well as bootstrapped mean value of a clustering
entropy from all components of the cluster points, $L_z(D \;|\;\mathcal{K})$.

<<echo=false; cache=false; fig_pos="H"; out_width="5.0in"; label="clustering-data-dl"; fig_cap="Clustering Data Description Length.">>=
# plot cluster model description length from sampled clusterings
datamdl_by_cls = @from r in clust_dmdl_stats begin
    @orderby r.ClusteringSize
    @group r by r.ClusteringSize into g
    @select get(g.key) => DataFrame(
        EntropyAll    = map(e->e[:EntropyAll], g),
        EntropyOCS    = map(e->e[:EntropyOCS], g),
        ManifoldProjDL = map(e->e[:ManifoldProjDL], g),
        DataDL         = map(e->e[:DataDL], g),
    )
    @collect Dict
end;

ks = datamdl_by_cls |> keys |> collect |> sort!
AvgEntropyAllCI = hcat([bootci(datamdl_by_cls[k][:EntropyAll], α=0.05) for k in ks]...)
AvgDataDLCI     = hcat([bootci(datamdl_by_cls[k][:DataDL], α=0.05) for k in ks]...)
DataDLBP = Dict((Float64(k),convert(Array, v[:DataDL])) for (k,v) in datamdl_by_cls)
xs = convert(Vector{Float64}, ks)

draw(width=800, height=480) do opts
    boxplot(DataDLBP)
    plot!(xs, AvgDataDLCI[2,:], typ=:line, col=2)
    ciplot!(xs, AvgDataDLCI, col=5)
    plot!(xs, AvgEntropyAllCI[2,:], typ=:line, col=5)
    color!(1)
    PLplot.labels("Number of clusters", "Data Description Length", "")
    PLplot.legend(["Average Clustering Data DL, L(D|K)", "Average Clustering Entropy, L#dz#u(D|K)"], text_colors=Cint[2,5], pos = PLplot.POSITION_TOP | PLplot.POSITION_RIGHT)
end
@

Displayed pattern shows expected linear downward trend: when the number of
clusters increases in the clustering, the clustering MDL value tend to decrease.
This happens because clusters become more compact and encoding is better for
such clusters yielding smaller MDL value. There is only one discrepancy in
the clustering size interval of [20,30]. It can be attributed to large cluster
dimensionality variation for clustering with particular number of clusters.

Now, we can calculate clustering MDL for the dataset $D$ by combining two parts:
$L(\mathcal{K})$ and $L(D \;|\;\mathcal{K})$.
We predicted that with that two-parted linear manifold MDL, we would have
large MDL value when the clustering has few clusters, which is driven by large
\emph{data}-part of joint MDL value for small-sized clusterings,
as well as large MDL value when the clustering has many clusters, which is
driven by large \emph{model}-part MDL value for large-sized clusterings.
Between these MDL value extrema, we should have clustering with minimal MDL
value characterized by balancing model and data MDL parts.

<<echo=false; cache=false; fig_pos="H"; out_width="5.0in"; label="clustering-mdl"; fig_cap="Avarage Clustering MDL for different encoding constant values.">>=
# plot clustering mdl for various model encoding constant

PMS = [16,32,40,48,64]
draw(width=800, height=800) do opts
    PLplot.plssub(2,3)

    for pm in PMS
    k = pm/32

    AvgClusteringMDL = (k*convert(Vector{Float64}, totmodelmdl_by_cls[:ModelDL]).+AvgDataDLCI[2,:]) / (10^6)
    AvgClusteringMDLZ = (k*convert(Vector{Float64}, totmodelmdl_by_cls[:ModelDL]).+AvgEntropyAllCI[2,:]) / (10^6)
    AvgEntropyAll = AvgEntropyAllCI / (10^6)
    AvgDataDL = AvgDataDLCI / (10^6)
    ymin, ymax = extrema(vcat(AvgEntropyAll, AvgDataDL))
    ymin, ymax = min(ymin, minimum(AvgClusteringMDL)), max(ymax, maximum(AvgClusteringMDL))
    ymin, ymax = min(ymin, minimum(AvgClusteringMDLZ)), max(ymax, maximum(AvgClusteringMDLZ))

    v1,i1 = findmin(AvgClusteringMDL)
    v2,i2 = findmin(AvgClusteringMDLZ)
#     println("Encoding constatnts: Model = $(Int(32*k)), Data = 16")
#     println("Minimum clustering MDL is $(v1) Mbits for clustering size: $(Int(xs[i1]))")
#     println("Minimum clustering (entropy-based) MDL is $(v2) Mbits for clustering size: $(Int(xs[i2]))")

    plot(xs, AvgDataDL[2,:], typ=:line, col=2, ymin=ymin, ymax=ymax)
    ciplot!(xs, AvgDataDL, col=5)
    plot!(xs, AvgEntropyAll[2,:], typ=:line, col=5)
    ciplot!(xs, AvgEntropyAll)
    plot!(xs, AvgClusteringMDL, typ=:line, col=6)
    PLplot.abline!(v=i1, col=3)
    PLplot.plmtex("bv", 2.2, (xs[i1]-3)/69, 0., "$i1")
    plot!(xs, AvgClusteringMDLZ, typ=:line, col=7)
    PLplot.abline!(v=i2, col=4)
    PLplot.plmtex("bv", 2.2, (xs[i2]-3)/69, 0., "$i2")
    color!(1)
    PLplot.labels("Number of clusters", "MDL, Mbits", "Average Clustering MDL, P#dm#u = $(Int(32*k)), P#dd#u = 16")
    end
    PLplot.pladv(0)
    PLplot.legend(["L(D|K)", "L#dz#u(D|K)", "L(K)+L(D|K)", "L(K)+L#dz#u(D|K)", "min(L(K)+L(D|K))", "min(L(K)+L#dz#u(D|K))"], text_colors=Cint[2, 5, 6, 7, 3, 4], y_offset = .15, x_offset = .25, text_spacing = 3., text_scale = 1.3, pos = PLplot.POSITION_TOP | PLplot.POSITION_LEFT | PLplot.POSITION_SUBPAGE, bb_color=Cint(0), width=0.2)
end
@

Figure~\ref{fig:clustering-mdl} shows plots of average two-part MDL values and
data MDL values (as well as MDL value based on cluster entropy) for clusterings
of different size. Vertical lines on plots identify clustering sizes which are
corresponded to the minimal MDL value under particular parameters.
We generated these plots for various $P_m$ constants, in order to see how these
constant would affect total MDL value, as it is primary parameter for the model
MDL calculation.

We found that with default parameters $P_m = 32$ and $P_d = 16$, the clustering
MDL value levels on after clustering size 30, reaching minimum value for
the clustering with 51 cluster and then gradually increases. With $P_m < 32$,
the clustering MDL value continues to decrease and we are not able to determine
correctly minimal MDL as we have a little or no data for clustering size beyond
67 clusters. With $P_m > 32$, we could clearly identify the minimal MDL value
and see its increase as clustering size grows.

Notably, $P_m = 40$ lands minimum clustering MDL for clusterings with
33 clusters which is very close to the expected value of 34 climate classes from
the K{\"o}ppen-Geiger (KG) climate classification.

<<echo=false; cache=false; fig_pos="H"; out_width="5.0in"; label="clustering-mdl-cr"; fig_cap="Average clustering MDL compression ratio.">>=
# plot compression ratio
RAW = d*24*32
MDLCRatio = hcat([RAW ./ ((pm/32)*totmodelmdl_by_cls[:ModelDL].+AvgDataDLCI[2,:]) for pm in PMS]...)
draw() do opts
    plot(xs, MDLCRatio, typ=:line)
    color!(1)
    PLplot.labels("Number of clusters", "Average Compression Ratio", "")
    PLplot.legend(["P#dm#u=$pm" for pm in PMS], pos = PLplot.POSITION_TOP | PLplot.POSITION_LEFT, text_spacing=2.2)
end
@

Figure~\ref{fig:clustering-mdl-cr} shows average compression ration calulated
for various model encoding constants.
Using average clustering MDL, we can evaluate some of the parameters of
the MDL heuristics for LMCLUS algorithm such as the encoding constants,
$P_d$ and $P_m$, and the compression ratio. Compression ratio defines the ratio
between an uncompressed (raw) encoding of the dataset with constant precision,
$P_r$ and a clustering MDL  which is an encoding of the dataset using
a particular clustering partition. It's calculated as follows
$$CR = \frac{P_rNd}{MDL}$$
where $P_r = 32$ is a precision encoding constant for raw data, $N$ is dataset dimension, and $d$ is a size of the dataset.
